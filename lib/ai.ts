import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI(process.env.GOOGLE_GEMINI_API_KEY || "");

export async function generateAIResponse(message: string, context: any = {}) {
  if (!process.env.GOOGLE_GEMINI_API_KEY) {
    console.error("CRITICAL: GOOGLE_GEMINI_API_KEY is missing from environment variables");
    return {
      text: "ğŸ¤– Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø°ÙƒÙŠ: Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… ÙŠØªÙ… ØªÙƒÙˆÙŠÙ† Ù…ÙØªØ§Ø­ Ø§Ù„Ù€ AI Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„Ù…Ø¯ÙŠØ±.",
      confidence: 0
    };
  }

    try {
      const model = genAI.getGenerativeModel({ 
        model: "gemini-2.0-flash",
        generationConfig: {
          maxOutputTokens: 1000,
          temperature: 0.7,
        }
      });


    const systemPrompt = `
Ø£Ù†Øª "Ø§Ù„Ø®Ø¨ÙŠØ± Ø§Ù„Ø°ÙƒÙŠ" ÙˆÙ†Ø¸Ø§Ù… Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ø´Ø±ÙƒØ© Logistics Systems Pro. 

ØµÙØªÙƒ Ø§Ù„Ø´Ø®ØµÙŠØ©:
1. Ø§Ø­ØªØ±Ø§ÙÙŠ Ù„Ù„ØºØ§ÙŠØ©ØŒ Ø°ÙƒÙŠØŒ ÙˆØ¯ÙˆØ¯ØŒ ÙˆÙ…Ø¨Ø§Ø¯Ø±.
2. Ø®Ø¨ÙŠØ± ÙÙŠ Ù‚Ø·Ø§Ø¹Ø§Øª Ø§Ù„Ù„ÙˆØ¬Ø³ØªÙŠØ§Øª (Ø§Ù„Ø´Ø­Ù†ØŒ Ø§Ù„ØªØªØ¨Ø¹ØŒ Ø§Ù„ØªØ®Ø²ÙŠÙ†)ØŒ Ø§Ù„Ù…Ø­Ø§Ø³Ø¨Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ©ØŒ ÙˆØ¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ© (HR).
3. Ù„Ø§ ØªØ±Ø¯ Ø£Ø¨Ø¯Ø§Ù‹ Ø¨Ø±Ø¯ÙˆØ¯ Ø¢Ù„ÙŠØ© Ø¬Ø§ÙØ©. ØªÙØ§Ø¹Ù„ ÙƒØ£Ù†Ùƒ Ù…Ø³ØªØ´Ø§Ø± Ø®Ø¨ÙŠØ± ÙŠØ³Ø¹Ù‰ Ù„Ø±Ø§Ø­Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„.

Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø±Ø¯:
1. Ø§Ø¨Ø¯Ø£ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨ØªØ­ÙŠØ© Ù„Ø§Ø¦Ù‚Ø© (Ù…Ø«Ù„: Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Logistics Systems ProØŒ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ) Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡Ø°Ù‡ Ø£ÙˆÙ„ Ø±Ø³Ø§Ù„Ø©.
2. Ø§Ù„ØªÙ…ÙŠØ²: Ø¥Ø°Ø§ Ø³Ø£Ù„ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¹Ù† "Ø®Ø¯Ù…Ø§ØªÙ†Ø§"ØŒ Ù„Ø§ ØªØ³Ø±Ø¯ Ù‚Ø§Ø¦Ù…Ø© ÙÙ‚Ø·ØŒ Ø¨Ù„ Ø§Ø´Ø±Ø­ Ø¨Ø£Ø³Ù„ÙˆØ¨ ØªØ³ÙˆÙŠÙ‚ÙŠ Ø°ÙƒÙŠ ÙƒÙŠÙ Ù†Ø³Ù‡Ù„ Ù„Ù‡ Ø£Ø¹Ù…Ø§Ù„Ù‡.
3. Ø§Ù„Ù…Ø¹Ø±ÙØ©: Ø§Ø³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙˆØ¯Ø©. Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø©ØŒ Ù‚Ù„ Ø¨Ø°ÙƒØ§Ø¡: "Ù‡Ø°Ø§ Ø§Ø³ØªÙØ³Ø§Ø± Ø¯Ù‚ÙŠÙ‚ØŒ Ø³Ø£Ù‚ÙˆÙ… ÙÙˆØ±Ø§Ù‹ Ø¨ØªØ­ÙˆÙŠÙ„Ù‡ Ù„Ù…Ø´Ø±Ù Ø§Ù„Ù†Ø¸Ø§Ù… Ù„Ø¶Ù…Ø§Ù† Ø­ØµÙˆÙ„Ùƒ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§ÙÙŠØ©ØŒ Ø§Ù†ØªØ¸Ø±Ù†Ø§ Ù‚Ù„ÙŠÙ„Ø§Ù‹ ÙˆØ³ÙŠØªÙ… Ø§Ù„Ø±Ø¯ Ø¹Ù„ÙŠÙƒ."
4. Ù…Ù…Ù†ÙˆØ¹ Ø§Ù„ØªÙƒØ±Ø§Ø±: Ù„Ø§ ØªØ¶Ø¹ Ø£ÙŠ Ø¨Ø§Ø¯Ø¦Ø© Ù…Ø«Ù„ "ğŸ¤– Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø°ÙƒÙŠ:". Ø§Ø¨Ø¯Ø£ Ù†ØµÙƒ ÙÙˆØ±Ø§Ù‹.
5. Ø§Ù„Ù…Ø¨Ø§Ø¯Ø±Ø©: ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø±Ø¯ÙƒØŒ Ø§Ù‚ØªØ±Ø­ Ø®Ø·ÙˆØ© ØªØ§Ù„ÙŠØ© (Ù…Ø«Ù„: "Ù‡Ù„ ØªÙˆØ¯ Ù…Ø¹Ø±ÙØ© ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØ«Ø± Ø¹Ù† Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø´Ø­Ù†ØŸ").

Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…ØªØ§Ø­Ø©:
${JSON.stringify(context.knowledge_base || [], null, 2)}

ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:
${JSON.stringify(context.conversation_history || [], null, 2)}

Ø£Ø¬Ø¨ Ø§Ù„Ø¢Ù† Ø¨Ø£Ø³Ù„ÙˆØ¨Ùƒ Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ Ø§Ù„Ù…ØªÙ…ÙŠØ²:
`;

    let result;
      try {
        result = await model.generateContent([systemPrompt, message]);
      } catch (genError: any) {
        console.error("Primary model failed, trying fallback...", genError.message);
        const fallbackModel = genAI.getGenerativeModel({ model: "gemini-2.0-flash" });
        result = await fallbackModel.generateContent([systemPrompt, message]);
      }


    const response = await result.response;
    let text = response.text().trim();

    // Remove any forced prefixes if the model still adds them
    text = text.replace(/^ğŸ¤– Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø°ÙƒÙŠ:\s*/, "");
    text = text.replace(/^Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø°ÙƒÙŠ:\s*/, "");

    let confidence = 0.9;
    if (text.includes("ØªØ­ÙˆÙŠÙ„Ùƒ Ù„Ù…Ù…Ø«Ù„ Ø¨Ø´Ø±ÙŠ") || text.includes("Ù„Ø§ Ø£Ù…Ù„Ùƒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª") || text.includes("ØºÙŠØ± Ù…ØªØ£ÙƒØ¯")) {
      confidence = 0.4;
    }

    return { text, confidence };
  } catch (error: any) {
    console.error("Gemini API Error Details:", error.message);
    throw error; // Let the caller handle the fallback
  }
}

  export async function analyzeMessage(message: string) {
    try {
      const model = genAI.getGenerativeModel({ model: "gemini-2.0-flash" });
      const prompt = `
  Ø­Ù„Ù„ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„ØªØ§Ù„ÙŠØ© ÙˆØ£Ø¬Ø¨ Ø¨ØªÙ†Ø³ÙŠÙ‚ JSON ÙÙ‚Ø·:
  {
    "language": "ar" Ø£Ùˆ "en",
    "category": "technical" Ø£Ùˆ "financial" Ø£Ùˆ "service" Ø£Ùˆ "general",
    "urgency": "normal" Ø£Ùˆ "urgent" Ø£Ùˆ "critical",
    "request_human": true Ø¥Ø°Ø§ Ø·Ù„Ø¨ ØµØ±Ø§Ø­Ø© Ù…ÙˆØ¸Ù Ø£Ùˆ Ø¥Ù†Ø³Ø§Ù†ØŒ ÙˆØ¥Ù„Ø§ false
  }

  Ø§Ù„Ø±Ø³Ø§Ù„Ø©: "${message}"
  `;

      let result;
      try {
        result = await model.generateContent(prompt);
      } catch (genError: any) {
        console.error("Analysis Primary model failed, trying fallback...", genError.message);
        const fallbackModel = genAI.getGenerativeModel({ model: "gemini-2.0-flash" });
        result = await fallbackModel.generateContent(prompt);
      }

    
    const response = await result.response;
    const text = response.text();
    
    const jsonMatch = text.match(/\{[\s\S]*\}/);
    const jsonStr = jsonMatch ? jsonMatch[0] : text;
    return JSON.parse(jsonStr);
  } catch (error) {
    console.error("Analysis Error:", error);
    return {
      language: "ar",
      category: "general",
      urgency: "normal",
      request_human: false
    };
  }
}
