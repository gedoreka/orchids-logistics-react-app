import { GoogleGenerativeAI } from "@google/generative-ai";
import { knowledgeBase } from "@/ai-assistant/core/knowledge-base";
import { getSystemStats } from "@/ai-assistant/data/system-data";
import { generateResponse as generateSamLocalResponse } from "@/ai-assistant/core/response-generator";

const genAI = new GoogleGenerativeAI(process.env.GOOGLE_GEMINI_API_KEY || "");

export async function generateAIResponse(message: string, context: any = {}) {
  const userName = context.user_name || "Ø§Ù„Ø¹Ù…ÙŠÙ„";
  const userId = context.company_id?.toString() || "default";

  // 1. Try local Sam Engine first for speed and quota saving
  const localResponse = await generateSamLocalResponse(userId, userName, message);
  
  // If local engine is highly confident (greeting or direct match), return it
  if (localResponse.analysis.confidence > 0.8) {
    return {
      text: localResponse.text,
      confidence: localResponse.analysis.confidence,
      buttons: localResponse.buttons || []
    };
  }

  if (!process.env.GOOGLE_GEMINI_API_KEY) {
    return {
      text: localResponse.text,
      confidence: 0.5,
      buttons: localResponse.buttons
    };
  }

  try {
    const stats = context.company_id ? await getSystemStats(context.company_id) : null;
    
    const model = genAI.getGenerativeModel({ 
      model: "gemini-2.0-flash",
      generationConfig: {
        maxOutputTokens: 1000,
        temperature: 0.8,
      }
    });

    const systemPrompt = `
Ø£Ù†Øª "Ø³Ø§Ù…" (Sam)ØŒ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ ÙˆØ§Ù„Ø®Ø¨ÙŠØ± Ø§Ù„Ù…Ø­Ø§Ø³Ø¨ÙŠ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø§Ø³Ø¨ Ø§Ù„Ø°ÙƒÙŠ (Smart Accountant System).
Ø£Ù†Øª Ù„Ø³Øª Ù…Ø¬Ø±Ø¯ Ø±ÙˆØ¨ÙˆØªØŒ Ø¨Ù„ Ø´Ø±ÙŠÙƒ Ø°ÙƒÙŠØŒ Ø¯Ø§ÙØ¦ØŒ ÙˆÙ…ØªÙØ§Ø¹Ù„ ÙŠÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆØ§Ù„Ù…Ø²Ø§Ø¬.

Ù‚ÙˆØ§Ø¹Ø¯Ùƒ Ø§Ù„Ø°Ù‡Ø¨ÙŠØ©:
1. Ø§Ù„Ø´Ø®ØµÙŠØ©: ÙƒÙ† ÙˆØ¯ÙˆØ¯Ø§Ù‹ Ù„Ù„ØºØ§ÙŠØ©ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ© (Emojis) Ø¨Ø´ÙƒÙ„ Ø°ÙƒÙŠØŒ ÙˆÙ†Ø§Ø¯Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ø³Ù…Ù‡: ${userName}.
2. Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø­ÙŠØ©: Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù„Ù„Ù†Ø¸Ø§Ù… Ø¥Ø°Ø§ ØªÙ… ØªØ²ÙˆÙŠØ¯Ùƒ Ø¨Ù‡Ø§.
3. Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ: Ø§ÙÙ‡Ù… Ù…Ø²Ø§Ø¬ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…. Ø¥Ø°Ø§ ÙƒØ§Ù† ØºØ§Ø¶Ø¨Ø§Ù‹ØŒ ÙƒÙ† Ù…ØªØ¹Ø§Ø·ÙØ§Ù‹ Ø¬Ø¯Ø§Ù‹. Ø¥Ø°Ø§ ÙƒØ§Ù† Ø³Ø¹ÙŠØ¯Ø§Ù‹ØŒ Ø´Ø§Ø±ÙƒÙ‡ Ø§Ù„ÙØ±Ø­Ø©.
4. Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©: Ø§Ø³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:
${JSON.stringify(knowledgeBase, null, 2)}

Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø­ÙŠØ© Ù„Ù„Ø´Ø±ÙƒØ© Ø§Ù„Ø­Ø§Ù„ÙŠØ©:
${JSON.stringify(stats, null, 2)}

ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:
${JSON.stringify(context.conversation_history || [], null, 2)}

Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹:
- Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ù…Ù‚Ø¯Ù…Ø§Øª Ù…ÙƒØ±Ø±Ø© Ù…Ø«Ù„ "Ø¨ØµÙØªÙŠ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ".
- Ø§Ø¬Ø¹Ù„ Ø±Ø¯ÙˆØ¯Ùƒ ØªÙØ§Ø¹Ù„ÙŠØ©ØŒ ÙˆØ§Ù‚ØªØ±Ø­ Ø®Ø·ÙˆØ§Øª Ù‚Ø§Ø¯Ù…Ø©.
- Ø¥Ø°Ø§ Ø³Ø£Ù„Ùƒ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… "Ù…Ù† Ø£Ù†ØªØŸ" Ø£Ùˆ "Ù…Ø§ Ø§Ø³Ù…ÙƒØŸ"ØŒ Ø£Ø¬Ø¨ Ø¨Ø£Ù†Ùƒ "Ø³Ø§Ù…"ØŒ Ù…Ø³Ø§Ø¹Ø¯Ù‡Ù… Ø§Ù„Ø°ÙƒÙŠ.
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØºÙ†ÙŠ (Markdown) Ù„Ø¬Ø¹Ù„ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø¬Ù…ÙŠÙ„Ø© (Ø¬Ø¯Ø§ÙˆÙ„ØŒ Ù‚ÙˆØ§Ø¦Ù…ØŒ Ø®Ø· Ø¹Ø±ÙŠØ¶).
- Ø¥Ø°Ø§ ÙƒÙ†Øª ØºÙŠØ± Ù…ØªØ£ÙƒØ¯ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø§Ù„Ù…Ø±ÙÙ‚Ø© Ø£Ø¹Ù„Ø§Ù‡.
`;

    let result;
    try {
      result = await model.generateContent([systemPrompt, message]);
    } catch (genError: any) {
      console.error("Gemini model failed, falling back to Sam Engine:", genError.message);
      return {
        text: localResponse.text,
        confidence: 0.6,
        buttons: localResponse.buttons
      };
    }

    const response = await result.response;
    let text = response.text().trim();

    text = text.replace(/^ğŸ¤– Ø³Ø§Ù…:\s*/, "");
    text = text.replace(/^Ø³Ø§Ù…:\s*/, "");

    let confidence = 0.9;
    if (text.includes("ØªØ­ÙˆÙŠÙ„Ùƒ Ù„Ù…Ù…Ø«Ù„ Ø¨Ø´Ø±ÙŠ") || text.includes("Ù„Ø§ Ø£Ù…Ù„Ùƒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª") || text.includes("ØºÙŠØ± Ù…ØªØ£ÙƒØ¯")) {
      confidence = 0.4;
    }

    return { text, confidence, buttons: localResponse.buttons.length > 0 ? localResponse.buttons : (context.buttons || []) };
  } catch (error: any) {
    console.error("Gemini API Error, using Sam fallback:", error.message);
    return {
      text: localResponse.text,
      confidence: 0.5,
      buttons: localResponse.buttons
    };
  }
}

export async function analyzeMessage(message: string) {
  try {
    const model = genAI.getGenerativeModel({ model: "gemini-2.0-flash" });
    const prompt = `
    Ø­Ù„Ù„ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„ØªØ§Ù„ÙŠØ© ÙˆØ£Ø¬Ø¨ Ø¨ØªÙ†Ø³ÙŠÙ‚ JSON ÙÙ‚Ø·:
    {
      "language": "ar" Ø£Ùˆ "en",
      "category": "technical" Ø£Ùˆ "financial" Ø£Ùˆ "service" Ø£Ùˆ "general",
      "urgency": "normal" Ø£Ùˆ "urgent" Ø£Ùˆ "critical",
      "request_human": true Ø¥Ø°Ø§ Ø·Ù„Ø¨ ØµØ±Ø§Ø­Ø© Ù…ÙˆØ¸Ù Ø£Ùˆ Ø¥Ù†Ø³Ø§Ù†ØŒ ÙˆØ¥Ù„Ø§ false
    }

    Ø§Ù„Ø±Ø³Ø§Ù„Ø©: "${message}"
    `;

    let result;
    try {
      result = await model.generateContent(prompt);
    } catch (genError: any) {
      console.error("Analysis Primary model failed, trying fallback...", genError.message);
      const fallbackModel = genAI.getGenerativeModel({ model: "gemini-2.0-flash" });
      result = await fallbackModel.generateContent(prompt);
    }

    const response = await result.response;
    const text = response.text();
    
    const jsonMatch = text.match(/\{[\s\S]*\}/);
    const jsonStr = jsonMatch ? jsonMatch[0] : text;
    return JSON.parse(jsonStr);
  } catch (error) {
    console.error("Analysis Error:", error);
    return {
      language: "ar",
      category: "general",
      urgency: "normal",
      request_human: false
    };
  }
}
